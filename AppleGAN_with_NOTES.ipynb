{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL8VNcV_jeez"
      },
      "source": [
        "# Steps to retrieve training data set.\n",
        "For this tutorial we will create and train a GAN using iamges of apples.\n",
        "\n",
        "1. Navigate to [kaggle.com](https://www.kaggle.com/#)\n",
        "2. Select register and follow the flow to creating a new account\n",
        "3. Once registerd sign into your account\n",
        "4. Profile avatar at the top right to reveal the drop down\n",
        "5. Select settings\n",
        "6. Scroll down to API\n",
        "7. Select create new token\n",
        "8. Save the kaggle.json file which gets generated\n",
        "9. Upload the kaggle.json to the google Collab root file directory\n",
        "\n",
        "This kaggle.json will be used to auth our dataset download.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnF5Ey93JshP"
      },
      "outputs": [],
      "source": [
        " # ! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_F8VrgU7pyq"
      },
      "outputs": [],
      "source": [
        "\n",
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3sJoQBO7sul"
      },
      "outputs": [],
      "source": [
        "! cp kaggle.json ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhgKC-7h8BJo"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wVTOoDc9N-s",
        "outputId": "7f018075-daec-4d74-d211-20ee77b7ec3c"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d moltean/fruits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY8bIEO79guk",
        "outputId": "d77cc681-6d64-4bae-fd53-4f9b6e3ce899"
      },
      "outputs": [],
      "source": [
        "! unzip fruits.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A4YEOwb9-vn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def get_apple_images(root_directory):\n",
        "    \"\"\"\n",
        "    This function searches through the given root_directory and its subdirectories\n",
        "    for any image files contained in folders starting with 'apple_' and returns a list of their paths.\n",
        "\n",
        "    :param root_directory: The directory to start the search from.\n",
        "    :return: A list of paths to the image files.\n",
        "    \"\"\"\n",
        "    image_files = []\n",
        "    for root, dirs, files in os.walk(root_directory):\n",
        "        # Check if the current folder starts with 'apple_'\n",
        "        if os.path.basename(root).startswith('Apple '):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
        "                    image_files.append(os.path.join(root, file))\n",
        "    return image_files\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39jOwOLn_ZC1",
        "outputId": "2e959286-8135-483c-c122-1ca5931d6b84"
      },
      "outputs": [],
      "source": [
        "# Specify the root directory to search from\n",
        "root_dir = './fruits-360_dataset/fruits-360/Training'\n",
        "image_paths = get_apple_images(root_dir)\n",
        "\n",
        "print(image_paths)\n",
        "\n",
        "# If you need to print or otherwise use the list of image paths:\n",
        "for path in image_paths:\n",
        "    print(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMwYSWA5CWmJ",
        "outputId": "6fe3f608-7e43-47ee-b49e-52069290c55f"
      },
      "outputs": [],
      "source": [
        "# !pip install Pillow numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOJI-APwCYg4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_images_into_array(image_paths, target_size=(256, 256)):\n",
        "    \"\"\"\n",
        "    This function loads and resizes images from the given paths into numpy arrays.\n",
        "\n",
        "    :param image_paths: A list of image file paths.\n",
        "    :param target_size: A tuple (width, height) to which all images will be resized.\n",
        "    :return: A list of numpy arrays representing the images.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    for path in image_paths:\n",
        "        with Image.open(path) as img:\n",
        "            img_resized = img.resize(target_size)  # Resize the image\n",
        "            images.append(np.array(img_resized))\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqTVmK_wCeCa",
        "outputId": "13583716-b658-4dc7-b258-afdda24616c0"
      },
      "outputs": [],
      "source": [
        "train_images = load_images_into_array(image_paths)\n",
        "\n",
        "# Now image_arrays contains all the images as numpy arrays\n",
        "# Here's how you can print the shape of the first image (if any)\n",
        "if train_images:\n",
        "    print(train_images[0].shape)\n",
        "else:\n",
        "    print(\"No images found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSWydEAbRgcP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "46Ddq21DByJ0",
        "outputId": "3fb9732b-01cf-473f-e29e-4502d5c4edf1"
      },
      "outputs": [],
      "source": [
        "plt.imshow(train_images[8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install \"tensorflow[and-cuda]<2.16\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0kwzWk_B4Yb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7uSccAOB2f3"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import imageio\n",
        "\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPAHIE6bDADb"
      },
      "outputs": [],
      "source": [
        "# Batch and shuffle the data\n",
        "#train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5xVr5RGC_QW"
      },
      "outputs": [],
      "source": [
        "#The BUFFER_SIZE variable, set to 60000, is used as a parameter in the dataset shuffling process.\n",
        "BUFFER_SIZE = 60000\n",
        "\n",
        "# This defines how many data points (in our case images) the model processes at once during training\n",
        "BATCH_SIZE = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x-eGi97LytF"
      },
      "source": [
        "When you call tf.data.Dataset.shuffle(BUFFER_SIZE), TensorFlow takes the first BUFFER_SIZE elements of the dataset and puts them into a buffer. During each training iteration, it randomly selects an element from this buffer and replaces it with the next element from the dataset, if there are any left. This approach ensures that a randomly shuffled version of the dataset is fed into the model, without the need to shuffle the entire dataset in memory all at once.\n",
        "\n",
        "Performance and Memory Use: The size of BUFFER_SIZE can impact both the randomness of your data during training and the memory usage. A larger BUFFER_SIZE can provide better shuffling but requires more memory to maintain the buffer. Typically, you would set BUFFER_SIZE to the size of the dataset or slightly smaller, depending on memory constraints. In this case, setting it to 60000 may imply that your dataset is about that size or that you are attempting to shuffle as much of it as possible at once for thorough randomness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25EvNmsmUtHR"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    #image = tf.image.decode_jpeg(image, channels=1)  # Convert to grayscale\n",
        "    image = tf.image.decode_jpeg(image, channels=3) # Convert to RGB\n",
        "    image = tf.image.resize(image, [28, 28])\n",
        "    image = (image - 127.5) / 127.5  # Normalize to [-1, 1]\n",
        "    return image\n",
        "\n",
        "# Assuming you have a list of image paths\n",
        "train_images = [preprocess_image(path) for path in image_paths]\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnJi2l4XHkGB"
      },
      "source": [
        "**Dense Layer:**\n",
        "\n",
        "This is the first layer of the generator model.\n",
        "It takes a 100-dimensional noise vector as input.\n",
        "It has 7*7*256 units or neurons. The layer's output will be reshaped in subsequent layers to form a 3D structure suitable for convolution operations.\n",
        "use_bias=False indicates that no bias vector is added to the layer outputs.\n",
        "This layer essentially projects and reshapes the input noise vector into a format suitable for convolutional operations that follow.\n",
        "\n",
        "\n",
        "**BatchNormalization Layer:**\n",
        "\n",
        "Normalizes the activations of the previous layer at each batch, i.e., it applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n",
        "Helps to stabilize the learning process and reduces the number of training epochs required to train deep networks.\n",
        "\n",
        "**LeakyReLU Layer:**\n",
        "\n",
        "Applies the Leaky Rectified Linear Unit activation function.\n",
        "Allows a small, non-zero gradient when the unit is not active, which helps prevent dead neurons in the network.\n",
        "LeakyReLU is often preferred in GANs because it helps with gradient flow.\n",
        "\n",
        "**Reshape Layer:**\n",
        "\n",
        "Reshapes the output from the dense layer into a 3D tensor of shape (7, 7, 256).\n",
        "This is necessary to match the dimensions expected by the subsequent convolutional layers.\n",
        "\n",
        "**Conv2DTranspose Layers:**\n",
        "\n",
        "These layers perform the opposite of a convolution operation, upsampling the input to a higher resolution or dimension.\n",
        "The first Conv2DTranspose layer upsamples to (7, 7, 128).\n",
        "The second upsamples further to (14, 14, 64).\n",
        "The third and final Conv2DTranspose layer upsamples to the target resolution of (28, 28, 1), producing a single-channel (grayscale) image.\n",
        "strides=(1, 1) or (2, 2) control the upsampling factor.\n",
        "padding='same' ensures the output size is adjusted to keep the spatial dimensions consistent through convolution operations.\n",
        "use_bias=False in these layers means no bias vector is added to the outputs.\n",
        "The last Conv2DTranspose layer uses a tanh activation function to normalize the output pixels to the range [-1, 1].\n",
        "\n",
        "**More BatchNormalization and LeakyReLU Layers:**\n",
        "\n",
        "Added after each Conv2DTranspose layer (except the final one) to stabilize training and introduce non-linearity, enabling the model to learn more complex patterns.\n",
        "\n",
        "**Final Output:**\n",
        "\n",
        "The model outputs a 28x28 pixel RGB image (as indicated by the shape (None, 28, 28, 1)), where None stands for batch size, allowing the model to process multiple images in parallel during training.\n",
        "\n",
        "\n",
        "MORE:\n",
        "\n",
        "Use of Conv2DTranspose layers: These layers are commonly used in GANs for upscaling the input and generating images. In this context, they're used to progressively upscale a low-dimensional input (in this case, from a 100-dimensional noise vector) to a higher resolution image.\n",
        "\n",
        "Starting from a dense layer that reshapes into an image format: The code begins with a dense layer that outputs to a shape which is then reshaped into an image format (7x7x256). This is a typical approach in GAN generators to start generating an image from a flattened vector.\n",
        "\n",
        "Batch normalization and LeakyReLU activations: These are common in GANs to help stabilize training and avoid vanishing or exploding gradients.\n",
        "\n",
        "Output activation is tanh: The use of the tanh function in the last convolutional layer is a common practice in GANs for generating images. This is because tanh outputs values in the range [-1, 1], which is often used for normalized image data.\n",
        "\n",
        "Output shape is (None, 28, 28, 3): This indicates that the network outputs color images (with 3 channels for RGB) of size 28x28 pixels. The use of GANs is prevalent for image generation tasks.\n",
        "\n",
        "The initial input shape is (100,): This signifies that the network starts with a 100-dimensional noise vector, which is typical for GAN generators. The noise vector serves as a seed for image generation, allowing the network to produce varied images.\n",
        "\n",
        "The commented-out line before the final Conv2DTranspose layer suggests there was an option to output a single-channel image (possibly for grayscale images), but it has been modified to produce 3-channel RGB images instead.\n",
        "\n",
        "Overall, this code defines a GAN generator designed to create 28x28 color images from a 100-dimensional noise input, showcasing a typical architecture for generating images in a GAN setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8d0yFWIG5Dv"
      },
      "outputs": [],
      "source": [
        "def generator():\n",
        "    \"\"\"\n",
        "        The tf.keras.Sequential() constructor is used in TensorFlow's Keras API to create a linear stack of layers, forming a model.\n",
        "        The Sequential model is a simple, yet extremely useful way to build a neural network for a wide array of problems.\n",
        "        It allows you to construct models layer by layer in a step-by-step fashion.\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    #model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "    assert model.output_shape == (None, 28, 28, 3)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "0JsWX8ilHHlY",
        "outputId": "db3a89db-b5e0-403c-a54e-c603f4e137d0"
      },
      "outputs": [],
      "source": [
        "generatorModel = generator()\n",
        "\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generatorModel(noise, training=False)\n",
        "\n",
        "# Remove 'cmap' to display the RGB image\n",
        "plt.imshow((generated_image[0, :, :, :] + 1) / 2)  # Rescale pixel values back to [0, 1] range for displaying\n",
        "plt.axis('off')  # Optionally remove the axis for a cleaner visualization\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLEaTSYvh6fN"
      },
      "source": [
        "Define a convolutional neural network (CNN) using TensorFlow's Keras API. This CNN includes the following layers:\n",
        "\n",
        "Conv2D Layers: These layers perform the convolution operation that involves a filter or kernel that passes over the input image (or feature map from the previous layer), capturing spatial hierarchies and features. The input image is specified to have a shape of 28x28 pixels with 3 channels (likely RGB), indicating that the model expects color images of this size. Each Conv2D layer uses a 5x5 kernel with a stride of 2 in both directions and padding set to 'same', which ensures the output volume has the same dimensions as the input when divided by the stride.\n",
        "\n",
        "LeakyReLU Layers: These are activation functions defined as\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "=\n",
        "�\n",
        "�\n",
        "f(x)=αx for\n",
        "�\n",
        "<\n",
        "0\n",
        "x<0 and\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "=\n",
        "�\n",
        "f(x)=x for\n",
        "�\n",
        "≥\n",
        "0\n",
        "x≥0. The leaky version of ReLU allows a small, non-zero gradient when the unit is inactive and has been shown to help maintain the flow of gradients through the network during training.\n",
        "\n",
        "Dropout Layers: These layers randomly set a fraction (0.3 in this case) of the input units to zero at each update during training time, which helps prevent overfitting.\n",
        "\n",
        "Flatten Layer: This layer flattens the output of the previous convolutional layers to form a single long feature vector necessary for the dense layer that follows.\n",
        "\n",
        "Dense Layer: This is a fully connected layer that outputs one unit. Since it's not followed by a softmax or sigmoid activation function in the provided code, this configuration suggests the model might be used for regression or binary classification tasks where the output is a single scalar value.\n",
        "\n",
        "Given the structure and the output layer, this model appears to be set up for a binary classification or regression problem, depending on the final activation function and loss functions used elsewhere in the implementation (not shown in this snippet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2VpLSDRL6Ow"
      },
      "outputs": [],
      "source": [
        "def discriminator():\n",
        "    \"\"\"\n",
        "      The tf.keras.Sequential() constructor is used in TensorFlow's Keras API to create a linear stack of layers, forming a model.\n",
        "      The Sequential model is a simple, yet extremely useful way to build a neural network for a wide array of problems.\n",
        "      It allows you to construct models layer by layer in a step-by-step fashion.\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential()\n",
        "    #model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 3]))\n",
        "\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrNQgUHoMKcw",
        "outputId": "56cf15dd-9d19-4020-c125-9a1c9bbd28de"
      },
      "outputs": [],
      "source": [
        "discriminator = discriminator()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXp-t6A8MNzo"
      },
      "outputs": [],
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCEJbYAZMOqg"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QA4KXM9TMV4h"
      },
      "outputs": [],
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnGqRzWj3q3P"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1rzxSnOMaLQ"
      },
      "outputs": [],
      "source": [
        "# The Adam optimizer (Adaptive Moment Estimation)\n",
        "# The adam optimizer is a Stochastic Gradient Descent (SGD): This is the simplest and most straightforward optimization algorithm.\n",
        "# It updates the weights using a fixed learning rate. Variants of SGD with momentum or Nesterov acceleration are often used to converge faster.\n",
        "\n",
        "# having an optimizer is crucial for training the neural network effectively.\n",
        "# An optimizer is responsible for updating the weights of the network based on the gradients calculated during backpropagation,\n",
        "# in order to minimize the loss function.\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOZ5_xuUMy4i"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generatorModel,\n",
        "                                 discriminator=discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pRbOzrrMzhl"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# You will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fep1gOiBM1eR"
      },
      "outputs": [],
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generatorModel(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generatorModel.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generatorModel.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAHRNaV0M5Ii"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      # plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.imshow((predictions[i, :, :, :] + 1) / 2)\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg-67FlOM3ei"
      },
      "outputs": [],
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generatorModel,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generatorModel,\n",
        "                           epochs,\n",
        "                           seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "ru06CgydM8qd",
        "outputId": "3bd67e46-e0cd-4bd6-a037-bda9f3ec16d1"
      },
      "outputs": [],
      "source": [
        "train(train_dataset, EPOCHS)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
